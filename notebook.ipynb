{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "import spacy\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-it-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-it-en\")\n",
    "\n",
    "it_nlp = spacy.load(\"it_core_news_sm\")\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "MarianMTModel                                                --\n",
      "‚îú‚îÄMarianModel: 1-1                                           --\n",
      "‚îÇ    ‚îî‚îÄEmbedding: 2-1                                        41,154,048\n",
      "‚îÇ    ‚îî‚îÄMarianEncoder: 2-2                                    41,154,048\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                   (recursive)\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMarianSinusoidalPositionalEmbedding: 3-2         (262,144)\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-3                                  18,914,304\n",
      "‚îÇ    ‚îî‚îÄMarianDecoder: 2-3                                    41,154,048\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-4                                   (recursive)\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMarianSinusoidalPositionalEmbedding: 3-5         (262,144)\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                  25,224,192\n",
      "‚îú‚îÄLinear: 1-2                                                41,154,048\n",
      "=====================================================================================\n",
      "Total params: 209,278,976\n",
      "Trainable params: 208,754,688\n",
      "Non-trainable params: 524,288\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary \n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LONDON - The danger of a nuclear Armageddon during the war in Ukraine appeared closer than it had officially leaked until now.\n",
      " In October 2022, while Kiev's counter-offensive had achieved considerable results and seemed able to resume Crimea as well, the United States obtained intelligence information that Russia was specifically discussing the\n",
      " And Washington was increasingly concerned about how he would react from a military standpoint.\n",
      "\n",
      "\n",
      "Nouns:\n",
      "| pericolo ->  hazard  \n",
      "| guerra ->  war  \n",
      "| ottobre ->  October  \n",
      "| controffensiva ->  counteroffensive\n",
      "| considerevoli ->  substantial  \n",
      "| risultati ->  results  \n",
      "| grado ->  grade  \n",
      "| informazioni ->  information  \n",
      "| intelligence ->  intelligence  \n",
      "| ipotesi ->  assumptions  \n",
      "| arma ->  weapon  \n",
      "| tattica ->  tactics \n",
      "| forze ->  forces  \n",
      "| preoccupazione ->  concern  \n",
      "| modo ->  mode  \n",
      "| punto ->  point  \n",
      "| vista ->  view  \n",
      "\n",
      "Verbs:\n",
      "| apparso ->  appeared  \n",
      "| trapelato ->  trapelate\n",
      "| raggiunto ->  reached  \n",
      "| sembrava ->  It looked like\n",
      "| riprendere ->  resume  \n",
      "| ottennero ->  got  \n",
      "| discuteva ->  talked about \n",
      "| usare ->  use  \n",
      "| considerava ->  considered  \n",
      "| reagito ->  reaction  \n"
     ]
    }
   ],
   "source": [
    "def translate(text, model, tokenizer, extract_pos=False):\n",
    "    # Split input text into smaller bits to be able to translate longer texts \n",
    "    doc = it_nlp(text)\n",
    "    \n",
    "    translations = []\n",
    "    for s in doc.sents:\n",
    "        if len(s) == 1:\n",
    "            continue\n",
    "\n",
    "        # Tokenize the source text\n",
    "        inputs = tokenizer.encode(str(s), return_tensors=\"pt\")\n",
    "\n",
    "        # Perform the translation and decode the output\n",
    "        outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "        translated_text = tokenizer.decode(outputs[0])\n",
    "        translations.append(translated_text) \n",
    "\n",
    "    full_translation = \"\\n\".join(translations)\n",
    "    if extract_pos:\n",
    "        noun_pairs, verb_pairs = translate_pos(doc)\n",
    "        pos_output = \"\\nNouns:\\n\" + \"\\n\".join([f\"| {noun} -> {translation}\" for noun, translation in noun_pairs.items()])\n",
    "        pos_output += \"\\n\\nVerbs:\\n\" + \"\\n\".join([f\"| {verb} -> {translation}\" for verb, translation in verb_pairs.items()])\n",
    "        full_translation += \"\\n\\n\" + pos_output\n",
    "\n",
    "    return full_translation.replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "def translate_words(words, model, tokenizer):\n",
    "    # Tokenize the source words\n",
    "    inputs = tokenizer.batch_encode_plus(words, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Perform the translation and decode the output\n",
    "    outputs = model.generate(**inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "    translated_words = [tokenizer.decode(output) for output in outputs]\n",
    "\n",
    "    return translated_words\n",
    "\n",
    "def translate_pos(spacy_doc):\n",
    "    nouns = [token.text for token in spacy_doc if token.pos_ == \"NOUN\"]\n",
    "    verbs = [token.text for token in spacy_doc if token.pos_ == \"VERB\"]\n",
    "\n",
    "    # Translate each unique noun and verb\n",
    "    unique_nouns = list(set(nouns))\n",
    "    unique_verbs = list(set(verbs))\n",
    "    noun_translations = translate_words(unique_nouns, model, tokenizer)\n",
    "    verb_translations = translate_words(unique_verbs, model, tokenizer)\n",
    "\n",
    "    # Create dictionaries mapping the original words to their translations\n",
    "    noun_pairs = dict(zip(unique_nouns, noun_translations))\n",
    "    verb_pairs = dict(zip(unique_verbs, verb_translations))\n",
    "\n",
    "    # Replace the original words with their translations\n",
    "    translated_nouns = [noun_pairs[noun] for noun in nouns]\n",
    "    translated_verbs = [verb_pairs[verb] for verb in verbs]\n",
    "\n",
    "    return dict(zip(nouns, translated_nouns)), dict(zip(verbs, translated_verbs))\n",
    "\n",
    "text = \"\"\"\n",
    "LONDRA - Il pericolo di un Armageddon nucleare durante la guerra in Ucraina √® apparso pi√π vicino di quanto era trapelato ufficialmente fino ad ora. \n",
    "Nell'ottobre 2022, mentre la controffensiva di Kiev aveva raggiunto considerevoli risultati e sembrava in grado di riprendere anche la Crimea, gli Stati Uniti ottennero informazioni di intelligence secondo cui la Russia discuteva concretamente l'ipotesi di usare un'arma atomica tattica contro le forze ucraine.\n",
    "E Washington considerava con crescente preoccupazione in che modo avrebbe reagito dal punto di vista militare.\n",
    "\"\"\"\n",
    "\n",
    "translated_text = translate(text, model, tokenizer, extract_pos=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video). Plus shows support for streaming text.\n",
    "\n",
    "\n",
    "def print_like_dislike(x: gr.LikeData):\n",
    "    print(x.index, x.value, x.liked)\n",
    "\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)\n",
    "\n",
    "\n",
    "def add_file(history, file):\n",
    "    history = history + [((file.name,), None)]\n",
    "    return history\n",
    "\n",
    "\n",
    "def bot(history):\n",
    "    response = \"**That's cool!**\"\n",
    "    history[-1][1] = \"\"\n",
    "    for character in response:\n",
    "        history[-1][1] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"avatar.png\"))),\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            scale=4,\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter text and press enter, or upload an image\",\n",
    "            container=False,\n",
    "        )\n",
    "        btn = gr.UploadButton(\"üìÅ\", file_types=[\"image\", \"video\", \"audio\"])\n",
    "\n",
    "    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n",
    "        bot, chatbot, chatbot, api_name=\"bot_response\"\n",
    "    )\n",
    "    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)\n",
    "    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "    chatbot.like(print_like_dislike, None, None)\n",
    "\n",
    "\n",
    "demo.queue()\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
